{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e8f11b-1b72-44f4-ae55-d6ec0b7f11a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Python Notebooks in Microsoft Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61cd804-88d8-47b5-937e-a87e15e6e945",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Set configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7fd526-f200-4b24-9846-3f3587213f5b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {\n",
    "        \"name\": \"lh_fmdk_solution_config\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5e049-539d-4501-bc67-5fc403f27f0c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install deltalake==0.18.2 -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330e25f-433b-4dbe-97b4-3c8a68523441",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1872117-776e-4d57-bc1e-2e1ce6caf53a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from deltalake import write_deltalake\n",
    "import notebookutils as nbutils\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import sys\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24c56c-757b-4e13-9417-81db0526a5c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Logger function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a4a67-b2d4-428d-96b1-0c0afb7a685e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def get_stdout_logger(logger_name: str, silence_other_loggers=True) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Return a preconfigured and named stdout Logger object.\n",
    "\n",
    "    Args:\n",
    "        logger_name (str): Name of logger\n",
    "        silence_other_loggers (bool, optional): Silences other loggers. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: A named Logger object\n",
    "    \"\"\"\n",
    "    # Set general logging level for all loggers\n",
    "    logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "    if silence_other_loggers:\n",
    "        # Silence all other loggers\n",
    "        for name, logger in logging.root.manager.loggerDict.items():\n",
    "            if name != f\"{logger_name}\":\n",
    "                logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "    # Use a named logger\n",
    "    nb_logger = logging.getLogger(logger_name)\n",
    "    nb_logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Add handler and formattter\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(\"%(asctime)s %(levelname)-5s %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    handler.setFormatter(formatter)\n",
    "    nb_logger.addHandler(handler)\n",
    "    nb_logger.propagate = False\n",
    "\n",
    "    return nb_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af525bc7-2c94-4c96-9241-c8753e3788c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Function to read Excel file (with pandas) and return PyArrow.Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334eb8f-dba9-40fa-8b67-30248459a1fc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def excel_sheet_to_pyarrow_table(abfss_path: str, sheet_name: str, logger: logging.Logger=None,  **kwargs: Any) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Read a specific sheet from an Excel file.\n",
    "    \n",
    "    Args:\n",
    "        abfss_path: ABFSS path to the Excel file (.xlsx format)\n",
    "        sheet_name: Name of the sheet to read\n",
    "        \n",
    "    Returns:\n",
    "        PyArrow Table containing the sheet data\n",
    "        \n",
    "    Example:\n",
    "        >>> config_table = read_config_sheet(\n",
    "        ...     \"abfss://container@storage.dfs.core.windows.net/config/solution_config.xlsx\",\n",
    "        ...     \"workspaces\"\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read Excel sheet using pandas\n",
    "        df = pd.read_excel(\n",
    "            abfss_path,\n",
    "            sheet_name=sheet_name,\n",
    "            engine='openpyxl'  # Required for .xlsx files\n",
    "        )\n",
    "        \n",
    "        # Convert to PyArrow Table for efficient columnar storage\n",
    "        arrow_table = pa.Table.from_pandas(df)\n",
    "\n",
    "        if logger:\n",
    "            logger.info(f\"✓ Successfully read sheet '{sheet_name}' with {len(df)} rows\")\n",
    "            logger.info(f\"  Columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        return arrow_table\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File not found: {abfss_path}\")\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Sheet '{sheet_name}' not found in workbook: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading Excel file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92a786-fd1c-44ec-bdcb-b4d543997053",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Function to create Fabric Workspaces from PyArrow.Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb574b-769e-4096-967a-73cca1c801bc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def create_workspaces(workspaces_table: pa.Table, logger: logging.Logger = None) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Create workspaces from a PyArrow table and add workspace IDs as a new column.\n",
    "    \n",
    "    Args:\n",
    "        workspaces_table: PyArrow table with workspace configurations\n",
    "        capacity_id: Azure capacity ID for workspace creation\n",
    "        logger: Logger object for logging operations\n",
    "        \n",
    "    Returns:\n",
    "        PyArrow table with added 'workspace_id' column\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to list for iteration\n",
    "        workspace_list = workspaces_table.to_pylist()\n",
    "        \n",
    "        # List to hold the returned id values for workspaces\n",
    "        workspace_ids = []\n",
    "\n",
    "        # Iterate and create workspaces\n",
    "        for ix, workspace in enumerate(workspace_list):\n",
    "            try:\n",
    "                ws_id = fabric.create_workspace(\n",
    "                    display_name=workspace[\"workspace_name\"],\n",
    "                    capacity_id=workspace[\"capacity_id\"],\n",
    "                    description=workspace[\"workspace_description\"]\n",
    "                )\n",
    "                \n",
    "                time.sleep(3)\n",
    "                workspace_ids.append(ws_id)\n",
    "                \n",
    "                if logger:\n",
    "                    logger.info(f\"✓ Created Workspace - Name: {workspace['workspace_name']}, Id={ws_id}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Log error and append None for failed workspace creation\n",
    "                workspace_ids.append(None)\n",
    "                if logger:\n",
    "                    logger.error(f\"✗ Failed to create workspace '{workspace['workspace_name']}': {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"Error creating workspace '{workspace['workspace_name']}': {str(e)}\")\n",
    "        \n",
    "        # Add workspace_id column to the table\n",
    "        workspace_id_array = pa.array(workspace_ids)\n",
    "        updated_table = workspaces_table.add_column(workspaces_table.num_columns, 'workspace_id', workspace_id_array)\n",
    "        \n",
    "        if logger:\n",
    "            successful_creates = sum(1 for ws_id in workspace_ids if ws_id is not None)\n",
    "            logger.info(f\"✓ Workspace creation completed: {successful_creates}/{len(workspace_list)} successful\")\n",
    "        \n",
    "        return updated_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Critical error in create_workspaces function: {str(e)}\"\n",
    "        if logger:\n",
    "            logger.error(error_msg)\n",
    "        else:\n",
    "            print(error_msg)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9172c-361e-4458-8c87-971b8b9ed982",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_delta_abfss(table: pa.Table, schema:pa.Schema, abfss_path:str, logger: logging.Logger = None):\n",
    "    \"\"\"\n",
    "    Save a PyArrow table to Delta format in ABFSS.\n",
    "    \n",
    "    Args:\n",
    "        table : pyarrow.Table\n",
    "            The PyArrow table to save\n",
    "        schema : pyarrow.Schema\n",
    "            Schema for the Delta table\n",
    "        delta_path : str\n",
    "            ABFSS path (abfss://container@account.dfs.core.windows.net/path)\n",
    "    \"\"\"\n",
    "    # Ensure table matches schema\n",
    "    if table.schema != schema:\n",
    "        table = table.cast(schema)\n",
    "\n",
    "    # Use default credentials token\n",
    "    token = notebookutils.credentials.getToken('storage')\n",
    "    storage_options = {\n",
    "            \"bearer_token\": token,\n",
    "            \"use_fabric_endpoint\": \"true\"\n",
    "        }\n",
    "\n",
    "    # Write to Delta Lake\n",
    "    write_deltalake(\n",
    "        abfss_path,\n",
    "        table,\n",
    "        mode=\"overwrite\",\n",
    "        storage_options=storage_options\n",
    "    )\n",
    "\n",
    "    if logger:\n",
    "        logger.info(f\"Data written to {abfss_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1497382-af34-4dcd-8268-d8faafe42472",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Do the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647bee3c-73b7-4109-a8a2-e99f238db237",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Path to configuration file\n",
    "CONFIG_ABFSS_PATH = \"abfss://ws_fmdk_solution@onelake.dfs.fabric.microsoft.com/lh_fmdk_solution_config.Lakehouse/Files/config/python_solution_config.xlsx\"\n",
    "\n",
    "# Path to config table\n",
    "\n",
    "TABLE_ABFSS_PATH = \"abfss://ws_fmdk_solution@onelake.dfs.fabric.microsoft.com/lh_fmdk_solution_config.Lakehouse/Tables/workspaces\"\n",
    "\n",
    "#  workspace_name, workspace_description, capacity_id\n",
    "WORKSPACES_SCHEMA = pa.schema([\n",
    "       pa.field(\"workspace_name\", pa.string()),\n",
    "       pa.field(\"workspace_description\", pa.string()),\n",
    "       pa.field(\"capacity_id\", pa.string()),\n",
    "       pa.field(\"workspace_id\", pa.string())\n",
    "   ])\n",
    "\n",
    "# Get a stdout logger object\n",
    "nb_logger = get_stdout_logger(\"notebook_logger\")\n",
    "\n",
    "config_sheet_table = excel_sheet_to_pyarrow_table(CONFIG_ABFSS_PATH, \"workspaces\", nb_logger)\n",
    "\n",
    "new_table = create_workspaces(config_sheet_table, nb_logger)\n",
    "\n",
    "# Save the table using deltalake\n",
    "\n",
    "save_to_delta_abfss(new_table, WORKSPACES_SCHEMA, TABLE_ABFSS_PATH, nb_logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1939f-4e9b-4afd-b5b3-10d55f4aa9fd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG_ABFSS_PATH = \"abfss://ws_fmdk_solution@onelake.dfs.fabric.microsoft.com/lh_fmdk_solution_config.Lakehouse/Files/config/python_solution_config.xlsx\"\n",
    "\n",
    "# Get a stdout logger object\n",
    "nb_logger = get_stdout_logger(\"notebook_logger\")\n",
    "\n",
    "sheet_table = excel_sheet_to_pyarrow_table(CONFIG_ABFSS_PATH, \"workspaces\", nb_logger)\n",
    "\n",
    "workspace_list = sheet_table.to_pylist()\n",
    "\n",
    "for workspace in workspace_list:\n",
    "    ws_id = fabric.create_workspace(\n",
    "        display_name=workspace[\"workspace_name\"],\n",
    "        capacity_id=workspace[\"capacity_id\"],\n",
    "        description=workspace[\"workspace_description\"]\n",
    "        )\n",
    "\n",
    "    time.sleep(3)   \n",
    "\n",
    "    nb_logger.info(f\"Created Workspace - Name: {workspace['workspace_name']}, Id={ws_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5c53e-a9ce-438f-b09c-076c93ae5931",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "fmdk-fabutils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
