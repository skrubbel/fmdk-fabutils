{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ea9fd6",
   "metadata": {},
   "source": [
    "# Python Notebooks on Microsoft Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7bca1-d218-4010-94b5-dbae2ed71567",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import notebookutils\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Get authentication token using Fabric's credential helper\n",
    "access_token = notebookutils.credentials.getToken('storage')\n",
    "\n",
    "# Define storage options for authentication\n",
    "storage_options = {\n",
    "    \"bearer_token\": access_token, \n",
    "    \"use_fabric_endpoint\": \"true\"\n",
    "}\n",
    "\n",
    "# Your ABFSS path\n",
    "abfss_path = \"abfss://ws_fmdk_solution@onelake.dfs.fabric.microsoft.com/lh_fmdk_solution_config.Lakehouse/Files/sample_data/organizations.csv\"\n",
    "\n",
    "# Read CSV with semicolon delimiter into a Pandas DataFrame\n",
    "pandas_df = pd.read_csv(\n",
    "    abfss_path, \n",
    "    sep=';',  # Use semicolon as delimiter\n",
    "    storage_options=storage_options\n",
    ")\n",
    "\n",
    "# Convert the Pandas DataFrame to a PyArrow table\n",
    "arrow_table = pa.Table.from_pandas(pandas_df)\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Use DuckDB to query the PyArrow table directly\n",
    "# Note: We register the arrow_table as a source for DuckDB\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM arrow_table\n",
    "ORDER BY \"Index\"\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and convert result to a new PyArrow table\n",
    "result_arrow_table = con.execute(query).arrow()\n",
    "\n",
    "# Now you have a PyArrow table with the first 20 rows\n",
    "print(f\"Number of rows in result: {len(result_arrow_table)}\")\n",
    "print(f\"Columns: {result_arrow_table.column_names}\")\n",
    "\n",
    "# Display the result PyArrow table\n",
    "print(\"\\nResult PyArrow Table:\")\n",
    "print(result_arrow_table.to_pandas())\n",
    "\n",
    "# Optional: You can run additional DuckDB queries on the original arrow_table\n",
    "industry_count_query = \"\"\"\n",
    "SELECT \"Industry\", COUNT(*) as \"Organization Count\"\n",
    "FROM arrow_table\n",
    "GROUP BY \"Industry\"\n",
    "ORDER BY \"Organization Count\" DESC\n",
    "\"\"\"\n",
    "\n",
    "industry_counts = con.execute(industry_count_query).arrow()\n",
    "print(\"\\nOrganization counts by industry:\")\n",
    "print(industry_counts.to_pandas())"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
